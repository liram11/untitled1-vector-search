{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "    \n",
    "from vecsim_app.multilabel_classifier.inference import predict_categories, load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding {'input_ids': tensor([[  101,  1996,  2088,  2004, 20607,  2592,  2023,  3259, 15841,  1996,\n",
      "          6666,  1997,  7851,  1996,  2088,  2004,  2592,  1010,  2926,  1999,\n",
      "          1996,  2817,  1997,  1996,  6622,  1997,  2166,  1998, 26497,  1012,\n",
      "          3151,  2913,  8087,  3471,  2138,  2009,  2003,  3697,  2000,  6235,\n",
      "          2166,  1998, 26497,  1999,  3408,  1997,  3043,  1998,  2943,  1010,\n",
      "          2144,  2037,  4277,  2024,  9398,  2069,  2012,  1996,  3558,  4094,\n",
      "          1012,  2174,  1010,  2065,  3043,  1998,  2943,  1010,  2004,  2092,\n",
      "          2004,  2166,  1998, 26497,  1010,  2024,  2649,  1999,  3408,  1997,\n",
      "          2592,  1010,  6622,  2064,  2022,  2649, 10862,  2004,  2592,  3352,\n",
      "          2062,  3375,  1012,  1996,  3259,  7534,  2809, 19943,  4277,  1997,\n",
      "          2592,  1010,  9398,  2012,  3674,  9539,  1010,  2029,  2024,  2236,\n",
      "         22318,  1997, 11534,  2937,  1010, 16941,  7159,  2594,  1010,  1996,\n",
      "         10867,  7716, 18279,  7712,  1010,  8317,  1010,  9569,  1010,  1998,\n",
      "         11619,  6481,  1012,  2122,  2024,  2582,  2109,  2000,  6848,  1996,\n",
      "         21951,  1997,  2166,  1010, 26497,  1998,  2037,  6622,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "outputs SequenceClassifierOutput(loss=None, logits=tensor([[-0.2116,  0.5989,  0.5665, -1.3931, -1.4983, -0.5358, -1.2067, -1.2998,\n",
      "         -1.1770, -0.8611, -1.2948, -1.0593, -1.1328, -1.1047, -1.1251, -1.4083,\n",
      "         -1.0779, -0.8038, -0.7758, -1.0649, -1.4309, -0.8749, -1.5344, -1.4426,\n",
      "          0.6799, -1.3825,  0.2495, -0.0958, -0.5197, -1.2239, -0.6497,  0.5589,\n",
      "         -0.8161, -0.6386, -0.0257, -0.2643,  0.0379,  0.0555, -0.5279, -0.5312,\n",
      "          0.4457,  0.8699, -0.9222, -1.1529, -1.3844, -0.7278]],\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected indicator for 161 classes, but got 46",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m queries \u001b[39m=\u001b[39m [query, query]\n\u001b[1;32m      7\u001b[0m model, tokenizer, mlb \u001b[39m=\u001b[39m load_models(\u001b[39m'\u001b[39m\u001b[39m./checkpoint/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmlb.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m categories \u001b[39m=\u001b[39m predict_categories(queries, model, tokenizer, mlb)\n",
      "File \u001b[0;32m~/workspace/untitled1-vector-search/backend/vecsim_app/multilabel_classifier/inference.py:61\u001b[0m, in \u001b[0;36mpredict_categories\u001b[0;34m(queries, model, tokenizer, mlb, proba_threshold)\u001b[0m\n\u001b[1;32m     58\u001b[0m categories \u001b[39m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[0;32m---> 61\u001b[0m     cat, probs \u001b[39m=\u001b[39m predict_categories_on_single_text(\n\u001b[1;32m     62\u001b[0m         query, model, tokenizer, mlb, proba_threshold\u001b[39m=\u001b[39;49mproba_threshold\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     categories\u001b[39m.\u001b[39mappend(cat)\n\u001b[1;32m     67\u001b[0m categories \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(flatten(categories)))\n",
      "File \u001b[0;32m~/workspace/untitled1-vector-search/backend/vecsim_app/multilabel_classifier/inference.py:27\u001b[0m, in \u001b[0;36mpredict_categories_on_single_text\u001b[0;34m(text, model, tokenizer, mlb, proba_threshold)\u001b[0m\n\u001b[1;32m     24\u001b[0m predictions[np\u001b[39m.\u001b[39mwhere(probs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m proba_threshold)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n\u001b[0;32m---> 27\u001b[0m classes \u001b[39m=\u001b[39m mlb\u001b[39m.\u001b[39;49minverse_transform(predictions\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(classes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     classes \u001b[39m=\u001b[39m [CATEGORIES[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m classes[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m/srv/conda/envs/saturn/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:904\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.inverse_transform\u001b[0;34m(self, yt)\u001b[0m\n\u001b[1;32m    901\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m yt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_):\n\u001b[0;32m--> 904\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    905\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected indicator for \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m classes, but got \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    906\u001b[0m             \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_), yt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    908\u001b[0m     )\n\u001b[1;32m    910\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(yt):\n\u001b[1;32m    911\u001b[0m     yt \u001b[39m=\u001b[39m yt\u001b[39m.\u001b[39mtocsr()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected indicator for 161 classes, but got 46"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "The World as Evolving Information   This paper discusses the benefits of describing the world as information,\\nespecially in the study of the evolution of life and cognition. Traditional\\nstudies encounter problems because it is difficult to describe life and\\ncognition in terms of matter and energy, since their laws are valid only at the\\nphysical scale. However, if matter and energy, as well as life and cognition,\\nare described in terms of information, evolution can be described consistently\\nas information becoming more complex.\\n  The paper presents eight tentative laws of information, valid at multiple\\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\\npsychological, philosophical, and complexity principles. These are further used\\nto discuss the notions of life, cognition and their evolution.\\n\n",
    "\"\"\".lower()\n",
    "\n",
    "queries = [query, query]\n",
    "\n",
    "model, tokenizer, mlb = load_models('./checkpoint/', 'mlb.pkl')\n",
    "\n",
    "categories = predict_categories(queries, model, tokenizer, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence', 'Computers and Society']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('saturn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c1f51487d8a055fea90b37fa3b43d5a61376641bd8f503a1f79d6e81aa7dcb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
