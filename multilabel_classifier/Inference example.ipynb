{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "    \n",
    "from multilabel_inference import predict_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "checkpoint is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:213\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 960\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/checkpoint/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1053\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1054\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1055\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1056\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1057\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1058\u001b[0m     )\n\u001b[1;32m   1059\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1060\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1359\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, use_auth_token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1350\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1351\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1352\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1358\u001b[0m )\n\u001b[0;32m-> 1359\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1361\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:242\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    234\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: 5tVsPAV8w60bQdqAuK_a4)\n\nRepository Not Found for url: https://huggingface.co/checkpoint/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnilgai/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnilgai/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mThe World as Evolving Information   This paper discusses the benefits of describing the world as information,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mespecially in the study of the evolution of life and cognition. Traditional\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mstudies encounter problems because it is difficult to describe life and\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mcognition in terms of matter and energy, since their laws are valid only at the\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mphysical scale. However, if matter and energy, as well as life and cognition,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mare described in terms of information, evolution can be described consistently\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mas information becoming more complex.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  The paper presents eight tentative laws of information, valid at multiple\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mpsychological, philosophical, and complexity principles. These are further used\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mto discuss the notions of life, cognition and their evolution.\u001b[39m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnilgai/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnilgai/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m queries \u001b[39m=\u001b[39m [query, query]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnilgai/home/paulo/untitled1-vector-search/multilabel_classifier/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m inference(queries, \u001b[39m'\u001b[39;49m\u001b[39m../data/checkpoint/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmlb.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/untitled1-vector-search/multilabel_classifier/multilabel_inference.py:50\u001b[0m, in \u001b[0;36minference\u001b[0;34m(queries, multilabel_model_path, multilabel_binarizer_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference\u001b[39m(queries: List[\u001b[39mstr\u001b[39m], multilabel_model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m'\u001b[39m, multilabel_binarizer_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmlb.pickle\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     model, tokenizer, mlb \u001b[39m=\u001b[39m load_models(multilabel_model_path, multilabel_binarizer_path)\n\u001b[1;32m     52\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mflatten\u001b[39m(l):\n\u001b[1;32m     53\u001b[0m         \u001b[39mreturn\u001b[39;00m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m l \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n",
      "File \u001b[0;32m~/untitled1-vector-search/multilabel_classifier/multilabel_inference.py:41\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m(multilabel_model_path, multilabel_binarizer_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_models\u001b[39m(multilabel_model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m'\u001b[39m, multilabel_binarizer_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmlb.pickle\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     36\u001b[0m     model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     37\u001b[0m         multilabel_model_path,\n\u001b[1;32m     38\u001b[0m         problem_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(multilabel_binarizer_path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m     44\u001b[0m         mlb \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:551\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    550\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    553\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:403\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 403\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    404\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    405\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    406\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    407\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    408\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    410\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    411\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    412\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    413\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    414\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    417\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    431\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    432\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: checkpoint is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "The World as Evolving Information   This paper discusses the benefits of describing the world as information,\\nespecially in the study of the evolution of life and cognition. Traditional\\nstudies encounter problems because it is difficult to describe life and\\ncognition in terms of matter and energy, since their laws are valid only at the\\nphysical scale. However, if matter and energy, as well as life and cognition,\\nare described in terms of information, evolution can be described consistently\\nas information becoming more complex.\\n  The paper presents eight tentative laws of information, valid at multiple\\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\\npsychological, philosophical, and complexity principles. These are further used\\nto discuss the notions of life, cognition and their evolution.\\n\n",
    "\"\"\".lower()\n",
    "\n",
    "queries = [query, query]\n",
    "\n",
    "inference(queries, '../data/checkpoint/', 'mlb.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence', 'Computers and Society']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3449e-05, 7.2455e-05, 1.6538e-03, 2.9704e-03, 1.2745e-03, 5.9530e-03,\n",
       "        3.5339e-03, 1.1511e-02, 5.9970e-05, 6.2610e-05, 3.0282e-04, 6.1172e-04,\n",
       "        1.1068e-03, 3.7223e-04, 1.6926e-04, 4.7154e-04, 5.2130e-04, 2.3101e-04,\n",
       "        4.6351e-04, 3.2123e-03, 1.4699e-04, 4.1342e-04, 4.7589e-04, 5.4869e-04,\n",
       "        7.6009e-04, 4.2186e-04, 2.6670e-03, 1.4211e-03, 2.8450e-04, 4.0546e-04,\n",
       "        7.6021e-04, 4.4429e-04, 7.0078e-04, 3.4472e-04, 4.2921e-04, 5.2881e-05,\n",
       "        4.8220e-04, 4.7684e-04, 1.0585e-03, 7.7342e-04, 7.8746e-04, 2.4646e-03,\n",
       "        5.7657e-04, 7.0069e-04, 3.8890e-04, 2.1129e-04, 4.5917e-04, 1.1026e-03,\n",
       "        4.9283e-04, 1.5127e-04, 8.2940e-05, 1.5043e-04, 2.3977e-04, 1.7363e-03,\n",
       "        2.2842e-04, 5.2750e-04, 6.0583e-04, 7.8573e-04, 9.6796e-04, 1.1842e-04,\n",
       "        2.8261e-04, 9.0861e-05, 4.0606e-04, 7.0971e-04, 1.0506e-03, 4.8595e-04,\n",
       "        6.7392e-04, 1.1234e-03, 2.0729e-04, 9.6482e-04, 3.9547e-04, 2.1310e-04,\n",
       "        9.0049e-04, 1.3816e-03, 3.5118e-04, 1.3193e-03, 4.5124e-04, 9.0196e-04,\n",
       "        7.2597e-04, 6.5003e-04, 3.4015e-04, 1.0533e-03, 7.3432e-04, 2.3656e-04,\n",
       "        8.6668e-04, 9.5088e-04, 5.7975e-04, 3.3815e-04, 4.5607e-04, 5.4863e-04,\n",
       "        4.6905e-04, 7.2141e-04, 1.4989e-04, 6.0129e-04, 5.9494e-04, 3.4867e-04,\n",
       "        8.2064e-04, 2.7855e-04, 4.9931e-04, 7.0954e-04, 5.7620e-04, 3.1191e-04,\n",
       "        2.2632e-04, 2.7939e-04, 4.6058e-04, 4.4094e-04, 1.1338e-04, 4.0174e-04,\n",
       "        1.0045e-04, 1.0481e-03, 1.0419e-03, 4.3936e-04, 9.6142e-04, 1.3279e-03,\n",
       "        2.5651e-04, 6.7573e-04, 4.3310e-04, 4.3300e-04, 4.0114e-04, 5.7360e-04,\n",
       "        2.0704e-03, 5.4480e-04, 7.3503e-04, 6.7376e-04, 1.4520e-03, 4.8389e-04,\n",
       "        1.3093e-03, 1.1474e-03, 1.1899e-03, 1.1834e-03, 5.2614e-04, 1.2371e-03,\n",
       "        1.6190e-03, 6.2887e-05, 1.7399e-04, 2.0572e-04, 1.9011e-04, 3.2003e-04,\n",
       "        8.8796e-04, 6.6481e-05, 6.9409e-04, 6.5904e-04, 1.2313e-04, 1.8524e-04,\n",
       "        1.2628e-04, 2.8709e-04, 1.6989e-04, 1.2548e-04, 7.6390e-05, 1.1459e-04,\n",
       "        1.1645e-04, 1.7771e-04, 1.1367e-04, 5.5165e-04, 5.3545e-05, 1.3432e-03,\n",
       "        3.2250e-04, 4.9289e-04, 8.3704e-04, 1.0918e-04, 2.1761e-04],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
